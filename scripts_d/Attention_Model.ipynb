{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_LEVELS = int(2.65 * 1000 )\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.linear = torch.nn.Linear(300,300, bias = True, dtype=torch.float32)\n",
    "        \n",
    "        self.emb_lvl = torch.nn.Embedding(SEP_LEVELS,300)\n",
    "        #self.linear_attention = torch.nn.Linear(300,300, dtype=torch.float32)\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size=300, hidden_size=128,batch_first = True, bidirectional=True) #dropout=.3,num_layers=2\n",
    "        self.linear_1 = torch.nn.Linear(in_features=128, out_features = 1, bias = True)\n",
    "        \n",
    "    def forward(self,x,previous_y):\n",
    "                        \n",
    "        # pass pipe pressures lags through Dense 300 ->300\n",
    "        after_linear = torch.nn.functional.relu( self.linear(x) )\n",
    "        \n",
    "        # get previous separator level embedding\n",
    "        previous_separator_level = round(previous_y ,3) * 1000      # previous_y = y[0]\n",
    "        previous_separator_level = int( previous_separator_level ) \n",
    "        previous_separator_level = torch.tensor( [ previous_separator_level ] )\n",
    "        previous_separator_level_emb = self.emb_lvl(previous_separator_level)     # Делаем ему размер emb = 300\n",
    "        previous_separator_level_emb = previous_separator_level_emb.reshape(1,300)\n",
    "        \n",
    "        # get attention\n",
    "        attention = after_linear * previous_separator_level_emb\n",
    "        #attention = self.linear_attention(attention)\n",
    "        attention_softmax = torch.nn.functional.softmax( attention , dim=0 )\n",
    "        \n",
    "        # multiply presure lags with attention softmax \n",
    "        final_state = x * attention_softmax\n",
    "        final_state = final_state.reshape(1,5,300)\n",
    "        \n",
    "        # LSTM\n",
    "        embeddings, (shortterm, longterm) = self.lstm(final_state)\n",
    "        longterm = torch.add(longterm[0],longterm[1])/2\n",
    "        predict = self.linear_1(longterm)\n",
    "        \n",
    "        return predict\n",
    "        \n",
    "    def fit(self, X_features, y_target):\n",
    "    \n",
    "        EPOCHS = 50 \n",
    "        criteria = torch.nn.HuberLoss()\n",
    "        ONE_EPOCH_SIZE = len(y_target) - 5\n",
    "        min_loss = 0.2\n",
    "        true_array = torch.from_numpy( y_target[5:]).float()\n",
    "        lr= 0.001\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            prediction_array = torch.tensor( [] ).float()\n",
    "\n",
    "            for i in tqdm(  range(5, len(y),1 )  ):\n",
    "                y_pred_array = torch.tensor([])\n",
    "                for j in range(3):\n",
    "                    x = np.array(X_features.iloc[i-5:i,j*300:(j+1)*300]) \n",
    "                    x = torch.from_numpy(x).reshape(5,300).float()\n",
    "\n",
    "                    previous_y = y_target[i]\n",
    "\n",
    "                    y_pred = self.forward( x,previous_y )\n",
    "                    y_pred_array = torch.cat((y_pred_array,y_pred) )\n",
    "                y_pred = torch.mean(y_pred_array).reshape(-1)\n",
    "\n",
    "                prediction_array = torch.cat((prediction_array,y_pred), dim=0)\n",
    "\n",
    "            loss = criteria( prediction_array.reshape(-1), true_array)\n",
    "\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "                torch.save(net,f'./model_loss_{loss}')\n",
    "                lr = 0.0001\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer = torch.optim.Adam( net.parameters(), lr = lr)\n",
    "\n",
    "\n",
    "            optimizer.step() # обновляем веса модели\n",
    "            optimizer.zero_grad()  # обнуляем веса\n",
    "\n",
    "            # at the end of each epoch print the last loss\n",
    "            if epoch % 1 ==0: print(f'epoch {epoch}. MSELoss = {loss}')\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self,X_features):\n",
    "        prediction_array = np.array([0.],dtype=np.float32)\n",
    "\n",
    "        #with torch.no_grad():\n",
    "        for i in tqdm( range(5, len(X_features), 1 ) ): \n",
    "            y_pred_array = torch.tensor([])\n",
    "            for j in range(3):\n",
    "                x = np.array(X_features.iloc[i-5:i , j*300:(j+1)*300]) \n",
    "                x = torch.from_numpy(x).reshape(5,300).float()\n",
    "\n",
    "                previous_y = prediction_array[-1]\n",
    "                \n",
    "                y_pred = self.forward( x,previous_y )\n",
    "                y_pred_array = torch.cat((y_pred_array,y_pred) )\n",
    "            y_pred = abs(torch.mean(y_pred_array))\n",
    "\n",
    "            prediction_array = np.append( prediction_array, y_pred.numpy() )\n",
    "        return prediction_array[1:]\n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
